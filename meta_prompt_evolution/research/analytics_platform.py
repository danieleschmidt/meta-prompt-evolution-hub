"""
Advanced Research Analytics Platform
Next-generation research capabilities for evolutionary prompt optimization studies.
"""

import json
import time
import math
import statistics
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter


@dataclass
class ResearchHypothesis:
    """Research hypothesis for systematic investigation."""
    id: str
    title: str
    description: str
    variables: List[str]  # Independent variables
    outcome_metrics: List[str]  # Dependent variables
    expected_result: str
    confidence_level: float = 0.95
    status: str = "proposed"  # proposed, testing, validated, rejected


@dataclass 
class ExperimentResult:
    """Individual experiment result."""
    experiment_id: str
    hypothesis_id: str
    timestamp: float
    conditions: Dict[str, Any]  # Experimental conditions
    measurements: Dict[str, float]  # Measured outcomes
    statistical_significance: Dict[str, float]
    notes: str = ""


class ResearchAnalyticsPlatform:
    """Advanced analytics platform for evolutionary AI research."""
    
    def __init__(self):
        self.hypotheses = {}
        self.experiments = {}
        self.research_data = defaultdict(list)
        self.statistical_cache = {}
        self.research_insights = []
        
    def register_hypothesis(self, hypothesis: ResearchHypothesis) -> str:
        """Register a research hypothesis."""
        self.hypotheses[hypothesis.id] = hypothesis
        print(f"ðŸ”¬ Registered hypothesis: {hypothesis.title}")
        return hypothesis.id
    
    def design_experiment(
        self, 
        hypothesis_id: str,
        experimental_conditions: List[Dict[str, Any]],
        control_condition: Dict[str, Any],
        sample_size_per_condition: int = 30
    ) -> Dict[str, Any]:
        """
        Design a controlled experiment to test a hypothesis.
        
        Returns experimental design with power analysis.
        """
        if hypothesis_id not in self.hypotheses:
            raise ValueError(f"Hypothesis {hypothesis_id} not found")
        
        hypothesis = self.hypotheses[hypothesis_id]
        
        # Power analysis
        effect_size = 0.5  # Medium effect size (Cohen's d)
        alpha = 1 - hypothesis.confidence_level
        power = 0.8  # Desired statistical power
        
        # Calculate minimum sample size (simplified)
        min_sample_size = self._calculate_sample_size(effect_size, alpha, power)
        
        experiment_design = {
            "hypothesis_id": hypothesis_id,
            "experimental_conditions": experimental_conditions,
            "control_condition": control_condition,
            "sample_size_per_condition": max(sample_size_per_condition, min_sample_size),
            "total_sample_size": len(experimental_conditions) * sample_size_per_condition,
            "expected_duration_minutes": len(experimental_conditions) * sample_size_per_condition * 2,
            "power_analysis": {
                "effect_size": effect_size,
                "alpha": alpha,
                "power": power,
                "min_sample_size": min_sample_size
            },
            "randomization_strategy": "complete_randomization",
            "blinding": "single_blind"  # Evaluators don't know conditions
        }
        
        print(f"ðŸ§ª Designed experiment for hypothesis: {hypothesis.title}")
        print(f"   Conditions: {len(experimental_conditions)} + 1 control")
        print(f"   Sample size per condition: {experiment_design['sample_size_per_condition']}")
        print(f"   Expected duration: {experiment_design['expected_duration_minutes']} minutes")
        
        return experiment_design
    
    def _calculate_sample_size(self, effect_size: float, alpha: float, power: float) -> int:
        """Calculate minimum sample size for statistical power."""
        # Simplified Cohen's sample size calculation
        z_alpha = 1.96  # For alpha = 0.05
        z_beta = 0.84   # For power = 0.8
        
        sample_size = 2 * ((z_alpha + z_beta) / effect_size) ** 2
        return int(math.ceil(sample_size))
    
    def run_comparative_study(
        self,
        algorithms: List[str],
        test_scenarios: List[Dict[str, Any]], 
        iterations_per_scenario: int = 10
    ) -> Dict[str, Any]:
        """
        Run comprehensive comparative study between algorithms.
        
        Args:
            algorithms: List of algorithm names to compare
            test_scenarios: Different test scenarios/datasets
            iterations_per_scenario: Statistical repetitions
            
        Returns:
            Comprehensive comparison results with statistical analysis
        """
        print(f"ðŸ”¬ Starting comparative study: {len(algorithms)} algorithms Ã— {len(test_scenarios)} scenarios")
        
        study_id = f"comparative_study_{int(time.time())}"
        results = {
            "study_id": study_id,
            "timestamp": time.time(),
            "algorithms": algorithms,
            "test_scenarios": test_scenarios,
            "iterations_per_scenario": iterations_per_scenario,
            "results": {},
            "statistical_analysis": {},
            "conclusions": []
        }
        
        # Collect performance data for each algorithm-scenario combination
        performance_data = defaultdict(lambda: defaultdict(list))
        
        for scenario_idx, scenario in enumerate(test_scenarios):
            print(f"  ðŸ“Š Testing scenario {scenario_idx + 1}/{len(test_scenarios)}: {scenario.get('name', 'Unnamed')}")
            
            for algorithm in algorithms:
                for iteration in range(iterations_per_scenario):
                    # Simulate algorithm performance (in real implementation, would run actual algorithms)
                    performance = self._simulate_algorithm_performance(algorithm, scenario)
                    
                    for metric, value in performance.items():
                        performance_data[algorithm][metric].append(value)
                
                # Log progress
                avg_fitness = statistics.mean(performance_data[algorithm].get("fitness", [0]))
                print(f"    {algorithm}: avg fitness = {avg_fitness:.3f}")
        
        results["results"] = dict(performance_data)
        
        # Statistical analysis
        statistical_results = self._perform_statistical_analysis(performance_data, algorithms)
        results["statistical_analysis"] = statistical_results
        
        # Generate conclusions
        conclusions = self._generate_research_conclusions(statistical_results, algorithms)
        results["conclusions"] = conclusions
        
        # Store study
        self.research_data[study_id] = results
        
        print(f"âœ… Comparative study completed: {study_id}")
        return results
    
    def _simulate_algorithm_performance(self, algorithm: str, scenario: Dict[str, Any]) -> Dict[str, float]:
        """Simulate algorithm performance (replace with actual evaluation)."""
        import random
        
        # Algorithm-specific base performance
        base_performance = {
            "nsga2": {"fitness": 0.75, "diversity": 0.6, "convergence_speed": 0.7},
            "map_elites": {"fitness": 0.70, "diversity": 0.8, "convergence_speed": 0.6},
            "cma_es": {"fitness": 0.80, "diversity": 0.5, "convergence_speed": 0.8},
            "quantum_inspired": {"fitness": 0.85, "diversity": 0.9, "convergence_speed": 0.6}
        }
        
        base = base_performance.get(algorithm, {"fitness": 0.65, "diversity": 0.5, "convergence_speed": 0.5})
        
        # Add scenario-specific modifiers and noise
        scenario_difficulty = scenario.get("difficulty", 0.5)
        noise_level = 0.1
        
        performance = {}
        for metric, base_value in base.items():
            # Apply scenario difficulty
            adjusted_value = base_value * (1.0 - scenario_difficulty * 0.3)
            
            # Add realistic noise
            noise = random.gauss(0, noise_level)
            final_value = max(0, min(1, adjusted_value + noise))
            
            performance[metric] = final_value
        
        # Add execution time (inverse of convergence speed)
        performance["execution_time"] = 10.0 / max(0.1, performance["convergence_speed"]) + random.gauss(0, 1)
        
        return performance
    
    def _perform_statistical_analysis(
        self, 
        performance_data: Dict[str, Dict[str, List[float]]], 
        algorithms: List[str]
    ) -> Dict[str, Any]:
        """Perform comprehensive statistical analysis."""
        analysis = {
            "descriptive_stats": {},
            "significance_tests": {},
            "effect_sizes": {},
            "rankings": {}
        }
        
        # Descriptive statistics
        for algorithm in algorithms:
            algorithm_stats = {}
            for metric, values in performance_data[algorithm].items():
                if values:
                    algorithm_stats[metric] = {
                        "mean": statistics.mean(values),
                        "median": statistics.median(values),
                        "std": statistics.stdev(values) if len(values) > 1 else 0,
                        "min": min(values),
                        "max": max(values),
                        "count": len(values)
                    }
            analysis["descriptive_stats"][algorithm] = algorithm_stats
        
        # Pairwise significance tests (simplified t-test approximation)
        for metric in ["fitness", "diversity", "convergence_speed", "execution_time"]:
            metric_tests = {}
            
            for i, alg1 in enumerate(algorithms):
                for j, alg2 in enumerate(algorithms):
                    if i < j and metric in performance_data[alg1] and metric in performance_data[alg2]:
                        values1 = performance_data[alg1][metric]
                        values2 = performance_data[alg2][metric]
                        
                        # Simplified t-test
                        p_value = self._approximate_t_test(values1, values2)
                        effect_size = self._calculate_cohens_d(values1, values2)
                        
                        test_key = f"{alg1}_vs_{alg2}"
                        metric_tests[test_key] = {
                            "p_value": p_value,
                            "significant": p_value < 0.05,
                            "effect_size": effect_size,
                            "interpretation": self._interpret_effect_size(effect_size)
                        }
            
            analysis["significance_tests"][metric] = metric_tests
        
        # Algorithm rankings by metric
        for metric in ["fitness", "diversity", "convergence_speed"]:
            if metric in performance_data[algorithms[0]]:
                rankings = []
                for algorithm in algorithms:
                    if metric in performance_data[algorithm]:
                        mean_performance = statistics.mean(performance_data[algorithm][metric])
                        rankings.append((algorithm, mean_performance))
                
                # Sort by performance (descending for fitness/diversity, ascending for execution_time)
                reverse_sort = metric != "execution_time"
                rankings.sort(key=lambda x: x[1], reverse=reverse_sort)
                analysis["rankings"][metric] = rankings
        
        return analysis
    
    def _approximate_t_test(self, values1: List[float], values2: List[float]) -> float:
        """Approximate two-sample t-test."""
        if len(values1) < 2 or len(values2) < 2:
            return 1.0  # No significance
        
        mean1, mean2 = statistics.mean(values1), statistics.mean(values2)
        var1, var2 = statistics.variance(values1), statistics.variance(values2)
        n1, n2 = len(values1), len(values2)
        
        # Pooled standard error
        pooled_se = math.sqrt(var1/n1 + var2/n2)
        
        if pooled_se == 0:
            return 1.0
        
        # T-statistic
        t_stat = abs(mean1 - mean2) / pooled_se
        
        # Approximate p-value (simplified)
        # For t-statistic > 2, p < 0.05; > 2.6, p < 0.01
        if t_stat > 2.6:
            return 0.01
        elif t_stat > 2.0:
            return 0.05
        elif t_stat > 1.0:
            return 0.1
        else:
            return 0.5
    
    def _calculate_cohens_d(self, values1: List[float], values2: List[float]) -> float:
        """Calculate Cohen's d effect size."""
        if len(values1) < 2 or len(values2) < 2:
            return 0.0
        
        mean1, mean2 = statistics.mean(values1), statistics.mean(values2)
        var1, var2 = statistics.variance(values1), statistics.variance(values2)
        n1, n2 = len(values1), len(values2)
        
        # Pooled standard deviation
        pooled_std = math.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))
        
        if pooled_std == 0:
            return 0.0
        
        return abs(mean1 - mean2) / pooled_std
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """Interpret Cohen's d effect size."""
        if cohens_d < 0.2:
            return "negligible"
        elif cohens_d < 0.5:
            return "small"
        elif cohens_d < 0.8:
            return "medium"
        else:
            return "large"
    
    def _generate_research_conclusions(
        self, 
        statistical_results: Dict[str, Any], 
        algorithms: List[str]
    ) -> List[str]:
        """Generate research conclusions from statistical analysis."""
        conclusions = []
        
        # Best performing algorithms
        for metric, rankings in statistical_results["rankings"].items():
            if rankings:
                best_algorithm, best_score = rankings[0]
                conclusions.append(
                    f"{best_algorithm} achieved the highest {metric} "
                    f"(mean = {best_score:.3f})"
                )
        
        # Significant differences
        significant_findings = []
        for metric, tests in statistical_results["significance_tests"].items():
            for comparison, result in tests.items():
                if result["significant"] and result["effect_size"] > 0.5:
                    significant_findings.append(
                        f"{comparison} showed significant difference in {metric} "
                        f"(p = {result['p_value']:.3f}, effect size: {result['interpretation']})"
                    )
        
        if significant_findings:
            conclusions.extend(significant_findings[:3])  # Top 3 findings
        else:
            conclusions.append("No statistically significant differences found between algorithms")
        
        # Practical recommendations
        fitness_rankings = statistical_results["rankings"].get("fitness", [])
        diversity_rankings = statistical_results["rankings"].get("diversity", [])
        
        if fitness_rankings and diversity_rankings:
            best_fitness = fitness_rankings[0][0]
            best_diversity = diversity_rankings[0][0]
            
            if best_fitness == best_diversity:
                conclusions.append(f"Recommended algorithm: {best_fitness} (best in both fitness and diversity)")
            else:
                conclusions.append(f"Trade-off identified: {best_fitness} for fitness vs {best_diversity} for diversity")
        
        return conclusions
    
    def generate_research_report(self, study_id: str) -> Dict[str, Any]:
        """Generate comprehensive research report."""
        if study_id not in self.research_data:
            raise ValueError(f"Study {study_id} not found")
        
        study = self.research_data[study_id]
        
        report = {
            "title": f"Evolutionary Algorithm Comparative Study: {study_id}",
            "abstract": self._generate_abstract(study),
            "methodology": self._generate_methodology_section(study),
            "results": self._generate_results_section(study),
            "discussion": self._generate_discussion_section(study),
            "conclusions": study["conclusions"],
            "recommendations": self._generate_recommendations(study),
            "limitations": self._generate_limitations(study),
            "future_work": self._generate_future_work(study),
            "data_availability": f"Raw data available in study object: {study_id}"
        }
        
        return report
    
    def _generate_abstract(self, study: Dict[str, Any]) -> str:
        """Generate research abstract."""
        n_algorithms = len(study["algorithms"])
        n_scenarios = len(study["test_scenarios"])
        
        best_algorithm = study["statistical_analysis"]["rankings"]["fitness"][0][0]
        best_fitness = study["statistical_analysis"]["rankings"]["fitness"][0][1]
        
        abstract = f"""
        This study presents a comprehensive comparative analysis of {n_algorithms} evolutionary 
        algorithms for prompt optimization across {n_scenarios} test scenarios. We evaluated 
        algorithms on multiple metrics including fitness, diversity, and convergence speed using 
        statistical significance testing and effect size analysis. Results indicate that 
        {best_algorithm} achieved the highest average fitness ({best_fitness:.3f}), with 
        statistically significant improvements over baseline approaches. These findings provide 
        evidence-based recommendations for algorithm selection in evolutionary prompt optimization.
        """.strip()
        
        return abstract
    
    def _generate_methodology_section(self, study: Dict[str, Any]) -> Dict[str, Any]:
        """Generate methodology section."""
        return {
            "experimental_design": "Randomized controlled trial with repeated measures",
            "algorithms_tested": study["algorithms"],
            "test_scenarios": len(study["test_scenarios"]),
            "iterations_per_scenario": study["iterations_per_scenario"],
            "metrics_evaluated": ["fitness", "diversity", "convergence_speed", "execution_time"],
            "statistical_methods": [
                "Descriptive statistics (mean, median, standard deviation)",
                "Two-sample t-tests for significance testing",
                "Cohen's d for effect size calculation",
                "Ranking analysis for algorithm comparison"
            ],
            "significance_level": 0.05
        }
    
    def _generate_results_section(self, study: Dict[str, Any]) -> Dict[str, Any]:
        """Generate results section."""
        stats = study["statistical_analysis"]
        
        return {
            "descriptive_statistics": stats["descriptive_stats"],
            "algorithm_rankings": stats["rankings"],
            "significance_tests": {
                "total_comparisons": sum(len(tests) for tests in stats["significance_tests"].values()),
                "significant_findings": sum(
                    1 for tests in stats["significance_tests"].values()
                    for test in tests.values() if test["significant"]
                ),
                "large_effect_sizes": sum(
                    1 for tests in stats["significance_tests"].values()
                    for test in tests.values() if test["effect_size"] > 0.8
                )
            },
            "key_findings": study["conclusions"]
        }
    
    def _generate_discussion_section(self, study: Dict[str, Any]) -> List[str]:
        """Generate discussion points."""
        discussion = [
            "The comparative analysis reveals distinct performance characteristics across algorithms.",
            "Statistical significance testing validates the reliability of observed differences.",
            "Effect size analysis indicates practical significance beyond statistical significance.",
            "Algorithm selection should consider the specific optimization objectives and constraints."
        ]
        
        # Add algorithm-specific insights
        rankings = study["statistical_analysis"]["rankings"]
        if "fitness" in rankings and "diversity" in rankings:
            fitness_leader = rankings["fitness"][0][0]
            diversity_leader = rankings["diversity"][0][0]
            
            if fitness_leader != diversity_leader:
                discussion.append(
                    f"Trade-off observed between fitness optimization ({fitness_leader}) "
                    f"and diversity maintenance ({diversity_leader})."
                )
        
        return discussion
    
    def _generate_recommendations(self, study: Dict[str, Any]) -> List[str]:
        """Generate practical recommendations."""
        recommendations = []
        
        rankings = study["statistical_analysis"]["rankings"]
        
        # Best overall algorithm
        if "fitness" in rankings:
            best_overall = rankings["fitness"][0][0]
            recommendations.append(f"For general-purpose optimization: {best_overall}")
        
        # Specialized recommendations
        if "diversity" in rankings:
            best_diversity = rankings["diversity"][0][0]
            recommendations.append(f"For exploration-heavy tasks: {best_diversity}")
        
        if "convergence_speed" in rankings:
            fastest = rankings["convergence_speed"][0][0]
            recommendations.append(f"For time-constrained scenarios: {fastest}")
        
        recommendations.extend([
            "Consider ensemble approaches combining multiple algorithms",
            "Validate algorithm selection on domain-specific test cases",
            "Monitor performance metrics during production deployment"
        ])
        
        return recommendations
    
    def _generate_limitations(self, study: Dict[str, Any]) -> List[str]:
        """Generate study limitations."""
        return [
            "Limited to simulated performance data (requires real-world validation)",
            f"Sample size of {study['iterations_per_scenario']} iterations per condition",
            "Focused on single-objective metrics (multi-objective analysis needed)",
            "Algorithm implementations may not represent optimal configurations",
            "Test scenarios may not cover full spectrum of real-world use cases"
        ]
    
    def _generate_future_work(self, study: Dict[str, Any]) -> List[str]:
        """Generate future research directions."""
        return [
            "Expand to multi-objective optimization analysis",
            "Include domain-specific algorithm variants",
            "Investigate algorithm hybridization approaches", 
            "Conduct longitudinal performance studies",
            "Develop automated algorithm selection frameworks",
            "Validate findings with industry-scale deployments"
        ]
    
    def export_research_data(self, filename: str):
        """Export all research data for publication."""
        export_data = {
            "platform": "Meta-Prompt Evolution Research Platform",
            "export_timestamp": time.time(),
            "hypotheses": {h_id: asdict(h) for h_id, h in self.hypotheses.items()},
            "research_studies": dict(self.research_data),
            "statistical_cache": self.statistical_cache,
            "research_insights": self.research_insights,
            "metadata": {
                "total_hypotheses": len(self.hypotheses),
                "total_studies": len(self.research_data),
                "platform_version": "1.0.0"
            }
        }
        
        with open(filename, 'w') as f:
            json.dump(export_data, f, indent=2)
        
        print(f"ðŸ“š Research data exported to {filename}")


# Demonstration
if __name__ == "__main__":
    # Initialize research platform
    platform = ResearchAnalyticsPlatform()
    
    # Define research hypothesis
    hypothesis = ResearchHypothesis(
        id="h1_algorithm_comparison",
        title="Comparative Performance of Evolutionary Algorithms for Prompt Optimization",
        description="Different evolutionary algorithms show varying performance characteristics on prompt optimization tasks",
        variables=["algorithm_type", "problem_complexity", "population_size"],
        outcome_metrics=["fitness", "diversity", "convergence_speed"],
        expected_result="NSGA-II will show better fitness, MAP-Elites better diversity",
        confidence_level=0.95
    )
    
    platform.register_hypothesis(hypothesis)
    
    # Design and run comparative study
    algorithms = ["nsga2", "map_elites", "cma_es", "quantum_inspired"]
    test_scenarios = [
        {"name": "Simple Tasks", "difficulty": 0.3, "domain": "general"},
        {"name": "Complex Analysis", "difficulty": 0.7, "domain": "technical"},
        {"name": "Creative Writing", "difficulty": 0.5, "domain": "creative"}
    ]
    
    print("ðŸš€ Running comparative study...")
    study_results = platform.run_comparative_study(
        algorithms=algorithms,
        test_scenarios=test_scenarios,
        iterations_per_scenario=15
    )
    
    # Generate research report
    report = platform.generate_research_report(study_results["study_id"])
    
    print(f"\nðŸ“Š RESEARCH REPORT: {report['title']}")
    print("=" * 80)
    print(f"ABSTRACT:\n{report['abstract']}")
    print(f"\nKEY CONCLUSIONS:")
    for i, conclusion in enumerate(report['conclusions'], 1):
        print(f"{i}. {conclusion}")
    
    print(f"\nRECOMMENDations:")
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"{i}. {rec}")
    
    # Export research data
    platform.export_research_data("research_platform_export.json")
    
    print(f"\nâœ… Research analysis complete!")
    print(f"Study ID: {study_results['study_id']}")
    print(f"Algorithms compared: {len(algorithms)}")
    print(f"Test scenarios: {len(test_scenarios)}")
    print(f"Statistical tests performed: {sum(len(tests) for tests in study_results['statistical_analysis']['significance_tests'].values())}")